# Legal Clause Similarity — Baselines (BiLSTM, Attentive BiGRU, TF-IDF)

This repository contains an end-to-end notebook implementation for a legal clause similarity task.
The goal is to determine whether two legal clauses are semantically similar (same legal meaning) or not.

The implementation is provided in the Jupyter Notebook `legal_clause_similarity.ipynb` and includes two neural baselines and a classical TF-IDF baseline. No pre-trained models were used.

## Repository contents

- `legal_clause_similarity.ipynb` — Main notebook. Contains data loading, clause-level splits, pair sampling, tokenization, three model implementations, training, evaluation and plotting.
- `nlp_dataset/` — Folder of CSV files, each file represents a clause category (e.g., `access.csv`, `arbitration.csv`), used to build clause pairs.
- `exported.md` — A short report exported by the notebook (generated when notebook runs).
- `roc_curves.png`, `pr_curves.png` — Plots generated by the notebook (after running evaluation cells).

## Problem: Legal Clause Similarity

Legal clauses are highly structured and may express the same legal principle in multiple ways. This project builds comparators that quantify semantic similarity between clause pairs. Two dimensions of similarity are considered:

- Semantic Equivalence — whether clauses express the same legal rule.
- Contextual Relatedness — whether clauses address related legal concepts.

Applications: contract analysis, de-duplication, legal retrieval, compliance checks.

## Dataset notes

- Each CSV file in `nlp_dataset/` groups clauses for a single category.
- The notebook reads clause text (column `clause_text`) and uses an in-file `clause_type` column when present.
- Clauses are split at the clause level (train/val/test) per category to avoid leakage; then positive (same category) and negative (different category) pairs are sampled within each split.

## Implemented baselines

1. BiLSTM Siamese
   - Embedding -> BiLSTM (bidirectional) -> mean pooling -> LayerNorm + MLP projection
   - Final head computes pairwise features: [u, v, |u-v|, u*v] -> MLP -> scalar logit

2. Attentive BiGRU Siamese
   - Embedding -> BiGRU (bidirectional) -> additive attention pooling (learned vector) -> dropout -> pairwise MLP head (same combination as above)

3. TF-IDF + Logistic Regression (classical baseline)
   - Concatenate `text_a [SEP] text_b` -> 1-2 gram TF-IDF -> liblinear logistic regression (C tuned on val set)

No pre-trained transformer or fine-tuned large language model was used by design.

## Key hyperparameters (as used in the notebook)

- Random seed: 67
- Max vocab: 30,000 (plus PAD & UNK)
- Max sequence length: 150 tokens
- Embedding dim: 128
- RNN hidden size: 128 (per direction) => representation dim = 256
- Dropout: 0.3
- Batch sizes: train 64, eval 128
- Optimizer: Adam
- LR: 1e-3
- Weight decay: 1e-4
- Epochs: up to 50 with early stopping (patience = 3)
- Gradient clipping: 2.0

These hyperparameters are declared and used in `legal_clause_similarity.ipynb` and can be tuned there.

## Evaluation metrics

The notebook reports the following metrics on the test set (after selecting the best threshold on validation):

- Accuracy (use when dataset is roughly balanced)
- Precision
- Recall
- F1-score (main metric for model selection)
- ROC-AUC and PR-AUC (for ranking and separation performance)

The notebook also prints confusion matrices and a classification report for qualitative inspection.

## How the notebook is organized (high-level)

1. Imports, device detection, and reproducibility setup
2. Clause-level loading & splitting (per category) and balanced pair sampling
3. Tokenization, vocabulary building and numericalization
4. PyTorch `PairDataset` and DataLoaders
5. Training utilities: BCEWithLogitsLoss, run_epoch, train_model (with early stopping), predict_probs, find_best_threshold
6. Model definitions: `BiLSTMSiamese` and `AttentiveGRUSiamese`
7. Train & evaluate BiLSTM
8. Train & evaluate Attentive BiGRU
9. TF-IDF + Logistic Regression baseline
10. Plots, exports (`exported.md`), ROC/PR curves and qualitative examples

Files produced when you run the full notebook (in working directory):

- `exported.md` — short report summary
- `roc_curves.png`, `pr_curves.png` — evaluation figures

## Reproduce (quick)

Prerequisites

Install the main Python packages (recommended to use a virtual environment):

```bash
python -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install torch pandas numpy scikit-learn matplotlib jupyterlab
```

Notes:
- Install a PyTorch build appropriate for your OS and CUDA/MPS availability; see https://pytorch.org for platform-specific commands.

Run the notebook

1. Start Jupyter in the repository root:

```bash
jupyter lab
```

2. Open `legal_clause_similarity.ipynb` and run the cells sequentially. The notebook will:
   - Build train/val/test clause pairs from `nlp_dataset/`
   - Train the two neural baselines and the TF-IDF baseline
   - Evaluate on the test set and save `exported.md`, ROC/PR plots

If you want a non-interactive run, you can execute the notebook with `papermill` or `nbconvert` — note that datasets and runtime environment (MPS/CUDA) may affect execution time.

## Expected outputs

- Console logs showing epoch-wise loss and validation F1
- Final metrics printed for BiLSTM, Attentive BiGRU, and TF-IDF
- `exported.md` report and ROC/PR plot images

## Observations (notebook's findings)

- The Attentive BiGRU tends to lead in F1/PR-AUC because attention helps focus on operative verbs and exceptions in longer clauses.
- BiLSTM can be faster to train and competitive on near-template boilerplate text.
- TF-IDF is a strong lexical baseline for near-duplicates but fails on paraphrase-level semantic similarity.

## Extending this work

- Add pre-trained contextual embeddings (e.g., transformer encoders) and compare performance (not allowed for current assignment requirement).
- Add cross-encoder / siamese transformers for stronger semantic modeling.
- Improve sampling strategy to ensure harder negatives and stratified label coverage.

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89cd1109",
   "metadata": {},
   "source": [
    "# Deep Learning Assignment 2\n",
    "\n",
    "Constructing two different baseline architectures: BiLSTM and Attention-pooled BiGRU to predict semantic similarity between legal clauses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77e560f",
   "metadata": {},
   "source": [
    "## 1. Imports & Global Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac144d81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T18:50:20.785509Z",
     "iopub.status.busy": "2025-11-08T18:50:20.784827Z",
     "iopub.status.idle": "2025-11-08T18:50:22.802743Z",
     "shell.execute_reply": "2025-11-08T18:50:22.802435Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, auc, confusion_matrix, classification_report, precision_recall_curve, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "SEED = 67\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "def _get_mps_preferred_device():\n",
    "    try:\n",
    "        mps = getattr(torch.backends, \"mps\", None)\n",
    "        if mps is not None and mps.is_available() and mps.is_built():\n",
    "            return torch.device(\"mps\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "device = _get_mps_preferred_device()\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5ef526",
   "metadata": {},
   "source": [
    "## 2. Clause-Level Splits & Pair Sampling\n",
    "\n",
    "We first load every clause text per category, split the _clauses_ into train/val/test buckets, and only then form positive/negative clause pairs within each split to avoid leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4806f13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause categories: 395; total clauses: 145,812\n",
      "Train split: 101,905 clauses across 394 labels\n",
      "Val split: 21,846 clauses across 394 labels\n",
      "Test split: 22,061 clauses across 394 labels\n",
      "Pair counts -> train: 203,810, val: 43,692, test: 44,122\n",
      "Negative/positive ratio: 1.0:1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_a</th>\n",
       "      <th>text_b</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Employee's employment is terminated without \"c...</td>\n",
       "      <td>that any Mortgage Loans become delinquent in t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>waiver of any term, provision or condition of ...</td>\n",
       "      <td>Agreement, any other Loan Document, nor any te...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>agreements herein, on the part of Lessee to be...</td>\n",
       "      <td>covenants herein set forth, the parties agree ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the Authorized Participant’s acceptance of an ...</td>\n",
       "      <td>the several paragraphs of this Agreement are i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(a) In the event a Change in Control of the Co...</td>\n",
       "      <td>Change in Control, any portion of the Restrict...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text_a  \\\n",
       "0  Employee's employment is terminated without \"c...   \n",
       "1  waiver of any term, provision or condition of ...   \n",
       "2  agreements herein, on the part of Lessee to be...   \n",
       "3  the Authorized Participant’s acceptance of an ...   \n",
       "4  (a) In the event a Change in Control of the Co...   \n",
       "\n",
       "                                              text_b  label  \n",
       "0  that any Mortgage Loans become delinquent in t...      1  \n",
       "1  Agreement, any other Loan Document, nor any te...      1  \n",
       "2  covenants herein set forth, the parties agree ...      1  \n",
       "3  the several paragraphs of this Agreement are i...      0  \n",
       "4  Change in Control, any portion of the Restrict...      1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Clause-level split + pair building ---\n",
    "DATA_DIR = Path('nlp_dataset')\n",
    "MIN_CLAUSES_PER_LABEL = 2\n",
    "NEG_POS_RATIO = 1.0\n",
    "POSITIVE_PAIRS_PER_CLAUSE = 1\n",
    "\n",
    "HEADER_PREFIX_RE = re.compile(r'^[a-z\\s\\.\\-]{1,40}:?\\s+', re.IGNORECASE)\n",
    "\n",
    "\n",
    "def strip_clause_heading(text: str) -> str:\n",
    "    text = (text or '').strip()\n",
    "    cleaned = HEADER_PREFIX_RE.sub('', text)\n",
    "    return cleaned if cleaned else text\n",
    "\n",
    "\n",
    "def normalize_clause(text: str) -> str:\n",
    "    stripped = strip_clause_heading(text)\n",
    "    stripped = re.sub(r'\\s+', ' ', stripped.lower())\n",
    "    return stripped\n",
    "\n",
    "\n",
    "rng = random.Random(SEED)\n",
    "\n",
    "\n",
    "# 1) Load clauses per label (prefer in-file clause_type column when present)\n",
    "label_to_texts = {}\n",
    "for file in sorted(DATA_DIR.glob('*.csv')):\n",
    "    df = pd.read_csv(file).dropna(subset=['clause_text'])\n",
    "    df['clause_text'] = df['clause_text'].astype(str).apply(strip_clause_heading)\n",
    "    df = df[df['clause_text'].str.len() > 0]\n",
    "    label_col = 'clause_type' if 'clause_type' in df.columns else None\n",
    "    if label_col:\n",
    "        for lab, group in df.groupby(label_col):\n",
    "            texts = group['clause_text'].tolist()\n",
    "            if len(texts) >= MIN_CLAUSES_PER_LABEL:\n",
    "                label_to_texts.setdefault(str(lab), []).extend(texts)\n",
    "    else:\n",
    "        if len(df) >= MIN_CLAUSES_PER_LABEL:\n",
    "            label_to_texts[file.stem] = df['clause_text'].tolist()\n",
    "\n",
    "seen = set()\n",
    "for lab, texts in list(label_to_texts.items()):\n",
    "    uniq = []\n",
    "    for text in texts:\n",
    "        key = normalize_clause(text)\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            uniq.append(text)\n",
    "    label_to_texts[lab] = uniq\n",
    "\n",
    "labels = sorted(label_to_texts)\n",
    "print(f\"Clause categories: {len(labels)}; total clauses: {sum(len(v) for v in label_to_texts.values()):,}\")\n",
    "\n",
    "\n",
    "# 2) Split at the clause level per label\n",
    "def split_texts(texts):\n",
    "    texts = list(texts)\n",
    "    if len(texts) < 4:\n",
    "        return texts, [], []\n",
    "    tr, temp = train_test_split(texts, test_size=0.3, random_state=SEED)\n",
    "    if len(temp) < 2:\n",
    "        return tr, temp, []\n",
    "    va, te = train_test_split(temp, test_size=0.5, random_state=SEED)\n",
    "    return tr, va, te\n",
    "\n",
    "\n",
    "split = {'train': {}, 'val': {}, 'test': {}}\n",
    "split_stats = {}\n",
    "for lab in labels:\n",
    "    tr, va, te = split_texts(label_to_texts[lab])\n",
    "    split['train'][lab] = tr\n",
    "    split['val'][lab] = va\n",
    "    split['test'][lab] = te\n",
    "\n",
    "for name, bucket in split.items():\n",
    "    clauses = sum(len(v) for v in bucket.values())\n",
    "    label_count = sum(1 for v in bucket.values() if len(v) > 0)\n",
    "    split_stats[name] = {'clauses': clauses, 'labels': label_count}\n",
    "    print(f\"{name.title()} split: {clauses:,} clauses across {label_count:,} labels\")\n",
    "\n",
    "\n",
    "# 3) Build balanced pairs within each split\n",
    "def build_pairs_for_split(bucket):\n",
    "    labs = [lab for lab, texts in bucket.items() if len(texts) > 0]\n",
    "    pos, neg = [], []\n",
    "    for lab in labs:\n",
    "        texts = bucket[lab]\n",
    "        if len(texts) < 2:\n",
    "            continue\n",
    "        for clause in texts:\n",
    "            for _ in range(POSITIVE_PAIRS_PER_CLAUSE):\n",
    "                partner = rng.choice(texts)\n",
    "                while partner == clause and len(texts) > 1:\n",
    "                    partner = rng.choice(texts)\n",
    "                pos.append((clause, partner, 1))\n",
    "    num_neg = int(len(pos) * NEG_POS_RATIO)\n",
    "    for _ in range(num_neg):\n",
    "        la, lb = rng.sample(labs, 2)\n",
    "        a = rng.choice(bucket[la]) if bucket[la] else None\n",
    "        b = rng.choice(bucket[lb]) if bucket[lb] else None\n",
    "        if a and b:\n",
    "            neg.append((a, b, 0))\n",
    "    pairs = pos + neg\n",
    "    rng.shuffle(pairs)\n",
    "    return pd.DataFrame(pairs, columns=['text_a', 'text_b', 'label'])\n",
    "\n",
    "\n",
    "def encode_split(bucket):\n",
    "    return build_pairs_for_split(bucket).reset_index(drop=True)\n",
    "\n",
    "\n",
    "train_df = encode_split(split['train'])\n",
    "val_df = encode_split(split['val'])\n",
    "test_df = encode_split(split['test'])\n",
    "pair_counts = {'train': len(train_df), 'val': len(val_df), 'test': len(test_df)}\n",
    "print(f\"Pair counts -> train: {pair_counts['train']:,}, val: {pair_counts['val']:,}, test: {pair_counts['test']:,}\")\n",
    "print(f\"Negative/positive ratio: {NEG_POS_RATIO}:1\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19110ebc",
   "metadata": {},
   "source": [
    "## 3. Tokenization & Numericalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4574b00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T18:50:24.020249Z",
     "iopub.status.busy": "2025-11-08T18:50:24.020174Z",
     "iopub.status.idle": "2025-11-08T18:50:24.278187Z",
     "shell.execute_reply": "2025-11-08T18:50:24.277733Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 30000\n",
      "Top tokens: [('the', 2981944), ('of', 1904838), ('or', 1458927), ('and', 1376200), ('to', 1313033), ('any', 891085), ('in', 863632), ('shall', 510768), ('be', 498965), ('a', 494305)]\n"
     ]
    }
   ],
   "source": [
    "TOKEN_PATTERN = re.compile(r\"[A-Za-z']+\")\n",
    "\n",
    "def tokenize(text: str):\n",
    "    return TOKEN_PATTERN.findall(text.lower())\n",
    "\n",
    "counter = Counter()\n",
    "for text in pd.concat([train_df['text_a'], train_df['text_b']]):\n",
    "    counter.update(tokenize(text))\n",
    "\n",
    "MAX_VOCAB = 30000\n",
    "most_common = counter.most_common(MAX_VOCAB - 2)\n",
    "vocab = {word: idx + 2 for idx, (word, _) in enumerate(most_common)}\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "MAX_SEQ_LEN = 150\n",
    "\n",
    "print('Vocab size:', len(vocab) + 2)\n",
    "print('Top tokens:', counter.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29979aac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T18:50:24.279447Z",
     "iopub.status.busy": "2025-11-08T18:50:24.279332Z",
     "iopub.status.idle": "2025-11-08T18:50:24.730502Z",
     "shell.execute_reply": "2025-11-08T18:50:24.730133Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded shapes -> (203810, 150) (43692, 150) (44122, 150)\n"
     ]
    }
   ],
   "source": [
    "def encode(text: str):\n",
    "    tokens = tokenize(text)\n",
    "    ids = [vocab.get(tok, UNK_IDX) for tok in tokens][:MAX_SEQ_LEN]\n",
    "    if len(ids) < MAX_SEQ_LEN:\n",
    "        ids += [PAD_IDX] * (MAX_SEQ_LEN - len(ids))\n",
    "    return ids\n",
    "\n",
    "\n",
    "def encode_dataframe(df):\n",
    "    left = np.array([encode(t) for t in df['text_a']], dtype=np.int64)\n",
    "    right = np.array([encode(t) for t in df['text_b']], dtype=np.int64)\n",
    "    labels = df['label'].astype(np.float32).values\n",
    "    return left, right, labels\n",
    "\n",
    "train_left, train_right, train_labels = encode_dataframe(train_df)\n",
    "val_left, val_right, val_labels = encode_dataframe(val_df)\n",
    "test_left, test_right, test_labels = encode_dataframe(test_df)\n",
    "print('Encoded shapes ->', train_left.shape, val_left.shape, test_left.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c9a25e",
   "metadata": {},
   "source": [
    "## 4. PyTorch Datasets & Loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0db7b97c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T18:50:24.732061Z",
     "iopub.status.busy": "2025-11-08T18:50:24.731986Z",
     "iopub.status.idle": "2025-11-08T18:50:24.734567Z",
     "shell.execute_reply": "2025-11-08T18:50:24.734189Z"
    }
   },
   "outputs": [],
   "source": [
    "class PairDataset(Dataset):\n",
    "    def __init__(self, left, right, labels):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.left[idx], dtype=torch.long),\n",
    "            torch.tensor(self.right[idx], dtype=torch.long),\n",
    "            torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "EVAL_BATCH_SIZE = 128\n",
    "max_workers = min(4, os.cpu_count() or 1)\n",
    "# Notebook-defined datasets cannot be pickled under spawn (macOS default), so default to serial loading.\n",
    "num_workers = 0\n",
    "loader_kwargs = {\n",
    "    'num_workers': num_workers,\n",
    "    'pin_memory': False,\n",
    "    'persistent_workers': False\n",
    "}\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    PairDataset(train_left, train_right, train_labels),\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    **loader_kwargs\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    PairDataset(val_left, val_right, val_labels),\n",
    "    batch_size=EVAL_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    **loader_kwargs\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    PairDataset(test_left, test_right, test_labels),\n",
    "    batch_size=EVAL_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    **loader_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87941cc1",
   "metadata": {},
   "source": [
    "## 5. Training Utilities & Early Stopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e38a587e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T18:50:24.735655Z",
     "iopub.status.busy": "2025-11-08T18:50:24.735567Z",
     "iopub.status.idle": "2025-11-08T18:50:24.739367Z",
     "shell.execute_reply": "2025-11-08T18:50:24.739035Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def _synchronize_if_needed():\n",
    "    if device.type == 'mps' and hasattr(torch, 'mps'):\n",
    "        try:\n",
    "            torch.mps.synchronize()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "def elapsed_since(start_time: float) -> float:\n",
    "    _synchronize_if_needed()\n",
    "    return time.time() - start_time\n",
    "\n",
    "\n",
    "def compute_metrics(probs, targets, threshold=0.5):\n",
    "    preds = (probs >= threshold).astype(int)\n",
    "    acc = accuracy_score(targets, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        targets, preds, average='binary', zero_division=0\n",
    "    )\n",
    "    try:\n",
    "        roc = roc_auc_score(targets, probs)\n",
    "    except ValueError:\n",
    "        roc = float('nan')\n",
    "    prec_curve, rec_curve, _ = precision_recall_curve(targets, probs)\n",
    "    pr_auc = auc(rec_curve, prec_curve)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc,\n",
    "        'pr_auc': pr_auc\n",
    "    }\n",
    "\n",
    "\n",
    "def find_best_threshold(probs, targets, default=0.5):\n",
    "    precision, recall, thresholds = precision_recall_curve(targets, probs)\n",
    "    if thresholds.size == 0:\n",
    "        return default\n",
    "    f1_scores = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "    candidate_scores = np.where(np.isfinite(f1_scores[:-1]), f1_scores[:-1], -np.inf)\n",
    "    best_idx = int(np.argmax(candidate_scores))\n",
    "    if candidate_scores[best_idx] == -np.inf:\n",
    "        return default\n",
    "    return float(thresholds[best_idx])\n",
    "\n",
    "\n",
    "def run_epoch(model, loader, optimizer=None):\n",
    "    if optimizer:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    total_loss = 0.0\n",
    "    probs_list, targets_list = [], []\n",
    "    for left, right, labels in loader:\n",
    "        left = left.to(device)\n",
    "        right = right.to(device)\n",
    "        labels = labels.to(device)\n",
    "        logits = model(left, right).squeeze(1)\n",
    "        loss = criterion(logits, labels)\n",
    "        if optimizer:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "            optimizer.step()\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        probs_list.append(torch.sigmoid(logits).detach().cpu().numpy())\n",
    "        targets_list.append(labels.detach().cpu().numpy())\n",
    "    probs_all = np.concatenate(probs_list)\n",
    "    targets_all = np.concatenate(targets_list)\n",
    "    metrics = compute_metrics(probs_all, targets_all)\n",
    "    return total_loss / len(loader.dataset), metrics\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=12, lr=1e-3, weight_decay=1e-4, patience=3):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    history = []\n",
    "    best_state, best_f1 = None, -1\n",
    "    epochs_without_improve = 0\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_metrics = run_epoch(model, train_loader, optimizer)\n",
    "        val_loss, val_metrics = run_epoch(model, val_loader)\n",
    "        history.append({\n",
    "            'epoch': epoch,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'train_metrics': train_metrics,\n",
    "            'val_metrics': val_metrics\n",
    "        })\n",
    "        if val_metrics['f1'] > best_f1 + 1e-4:\n",
    "            best_f1 = val_metrics['f1']\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            epochs_without_improve = 0\n",
    "        else:\n",
    "            epochs_without_improve += 1\n",
    "        print(f\"Epoch {epoch:02d} | train_loss={train_loss:.4f} val_loss={val_loss:.4f} val_f1={val_metrics['f1']:.4f}\")\n",
    "        if epochs_without_improve >= patience:\n",
    "            print('Early stopping triggered.')\n",
    "            break\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model, history\n",
    "\n",
    "\n",
    "def predict_probs(model, loader):\n",
    "    model.eval()\n",
    "    probs, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for left, right, y in loader:\n",
    "            left, right = left.to(device), right.to(device)\n",
    "            logits = model(left, right).squeeze(1)\n",
    "            probs.append(torch.sigmoid(logits).cpu().numpy())\n",
    "            targets.append(y.numpy())\n",
    "    return np.concatenate(probs), np.concatenate(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56711031",
   "metadata": {},
   "source": [
    "## 6. RNN Architectures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84d5c2af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T18:50:24.740361Z",
     "iopub.status.busy": "2025-11-08T18:50:24.740306Z",
     "iopub.status.idle": "2025-11-08T18:50:24.744088Z",
     "shell.execute_reply": "2025-11-08T18:50:24.743737Z"
    }
   },
   "outputs": [],
   "source": [
    "class BiLSTMSiamese(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_size=128, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD_IDX)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        rep_dim = hidden_size * 2\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.LayerNorm(rep_dim),\n",
    "            nn.Linear(rep_dim, rep_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(rep_dim * 4, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        mask = (x != PAD_IDX).unsqueeze(-1)\n",
    "        emb = self.embedding(x)\n",
    "        output, _ = self.lstm(emb)\n",
    "        output = output * mask\n",
    "        summed = output.sum(dim=1)\n",
    "        lengths = mask.sum(dim=1).clamp(min=1).float()\n",
    "        mean_pooled = summed / lengths\n",
    "        return self.projection(self.dropout(mean_pooled))\n",
    "\n",
    "    def forward(self, left, right):\n",
    "        left_vec = F.normalize(self.encode(left), dim=1)\n",
    "        right_vec = F.normalize(self.encode(right), dim=1)\n",
    "        combined = torch.cat([left_vec, right_vec, torch.abs(left_vec - right_vec), left_vec * right_vec], dim=1)\n",
    "        return self.head(combined)\n",
    "\n",
    "\n",
    "class AttentiveGRUSiamese(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_size=128, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD_IDX)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.attn_vector = nn.Parameter(torch.randn(hidden_size * 2))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        rep_dim = hidden_size * 2\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(rep_dim * 4, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        pad_mask = (x == PAD_IDX)\n",
    "        emb = self.embedding(x)\n",
    "        outputs, _ = self.gru(emb)\n",
    "        scores = torch.matmul(outputs, self.attn_vector)\n",
    "        scores = scores.masked_fill(pad_mask, -1e9)\n",
    "        attn = torch.softmax(scores, dim=1).unsqueeze(-1)\n",
    "        attn = torch.where(torch.isfinite(attn), attn, torch.zeros_like(attn))\n",
    "        context = torch.sum(outputs * attn, dim=1)\n",
    "        return self.dropout(context)\n",
    "\n",
    "    def forward(self, left, right):\n",
    "        left_vec = F.normalize(self.encode(left), dim=1)\n",
    "        right_vec = F.normalize(self.encode(right), dim=1)\n",
    "        combined = torch.cat([left_vec, right_vec, torch.abs(left_vec - right_vec), left_vec * right_vec], dim=1)\n",
    "        return self.head(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56ab9d6",
   "metadata": {},
   "source": [
    "## 7. Train & Evaluate – BiLSTM Siamese\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f51a90af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T18:50:24.745038Z",
     "iopub.status.busy": "2025-11-08T18:50:24.744974Z",
     "iopub.status.idle": "2025-11-08T18:56:54.311701Z",
     "shell.execute_reply": "2025-11-08T18:56:54.310309Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_loss=0.5404 val_loss=0.6213 val_f1=0.7926\n",
      "Epoch 02 | train_loss=0.4095 val_loss=0.4722 val_f1=0.8331\n",
      "Epoch 03 | train_loss=0.3662 val_loss=0.5473 val_f1=0.8169\n",
      "Epoch 04 | train_loss=0.3459 val_loss=0.5320 val_f1=0.8335\n",
      "Epoch 05 | train_loss=0.3322 val_loss=0.4469 val_f1=0.8493\n",
      "Epoch 06 | train_loss=0.3195 val_loss=0.5500 val_f1=0.8266\n",
      "Epoch 07 | train_loss=0.3111 val_loss=0.4666 val_f1=0.8452\n",
      "Epoch 08 | train_loss=0.3043 val_loss=0.4583 val_f1=0.8509\n",
      "Epoch 09 | train_loss=0.2963 val_loss=0.4831 val_f1=0.8407\n",
      "Epoch 10 | train_loss=0.2898 val_loss=0.4534 val_f1=0.8552\n",
      "Epoch 11 | train_loss=0.2847 val_loss=0.5193 val_f1=0.8463\n",
      "Epoch 12 | train_loss=0.2778 val_loss=0.4737 val_f1=0.8547\n",
      "Epoch 13 | train_loss=0.2712 val_loss=0.3964 val_f1=0.8666\n",
      "Epoch 14 | train_loss=0.2667 val_loss=0.3934 val_f1=0.8682\n",
      "Epoch 15 | train_loss=0.2618 val_loss=0.4336 val_f1=0.8616\n",
      "Epoch 16 | train_loss=0.2583 val_loss=0.4697 val_f1=0.8576\n",
      "Epoch 17 | train_loss=0.2558 val_loss=0.4130 val_f1=0.8680\n",
      "Early stopping triggered.\n",
      "\n",
      "BiLSTM Test Loss: 0.39299460406489506\n",
      "BiLSTM Test Metrics: {'accuracy': 0.8686142967227234, 'precision': 0.8579225352112676, 'recall': 0.88355015638457, 'f1': 0.8705477769589781, 'roc_auc': 0.9466416957332686, 'pr_auc': 0.9498477297270047, 'threshold': 0.6813130378723145}\n",
      "BiLSTM best threshold (val F1): 0.681\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "LR = 1e-3\n",
    "PATIENCE = 3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "histories = {}\n",
    "results = {}\n",
    "best_thresholds = {}\n",
    "\n",
    "y_test = test_labels\n",
    "\n",
    "bilstm_model = BiLSTMSiamese(len(vocab) + 2)\n",
    "start = time.time()\n",
    "bilstm_model, bilstm_history = train_model(\n",
    "    bilstm_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epochs=EPOCHS,\n",
    "    lr=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    patience=PATIENCE\n",
    ")\n",
    "bilstm_time = elapsed_since(start)\n",
    "bilstm_test_loss, _ = run_epoch(bilstm_model, test_loader)\n",
    "val_probs, val_targets = predict_probs(bilstm_model, val_loader)\n",
    "bilstm_threshold = find_best_threshold(val_probs, val_targets)\n",
    "bilstm_test_probs, _ = predict_probs(bilstm_model, test_loader)\n",
    "bilstm_test_metrics = compute_metrics(bilstm_test_probs, y_test, threshold=bilstm_threshold)\n",
    "bilstm_test_metrics['threshold'] = bilstm_threshold\n",
    "print(\"\\nBiLSTM Test Loss:\", bilstm_test_loss)\n",
    "print('BiLSTM Test Metrics:', bilstm_test_metrics)\n",
    "print(f\"BiLSTM best threshold (val F1): {bilstm_threshold:.3f}\")\n",
    "\n",
    "histories['BiLSTM'] = bilstm_history\n",
    "best_thresholds['BiLSTM'] = bilstm_threshold\n",
    "results['BiLSTM'] = {**bilstm_test_metrics, 'loss': bilstm_test_loss, 'train_time_sec': bilstm_time}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac415bc",
   "metadata": {},
   "source": [
    "## 8. Train & Evaluate – Attentive BiGRU Siamese\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8e70bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T18:56:54.314575Z",
     "iopub.status.busy": "2025-11-08T18:56:54.314154Z",
     "iopub.status.idle": "2025-11-08T19:00:57.838272Z",
     "shell.execute_reply": "2025-11-08T19:00:57.836749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_loss=0.6311 val_loss=0.5551 val_f1=0.7586\n",
      "Epoch 02 | train_loss=0.4144 val_loss=0.4635 val_f1=0.8327\n",
      "Epoch 03 | train_loss=0.3064 val_loss=0.3866 val_f1=0.8651\n",
      "Epoch 04 | train_loss=0.2709 val_loss=0.3502 val_f1=0.8721\n",
      "Epoch 05 | train_loss=0.2554 val_loss=0.3582 val_f1=0.8783\n",
      "Epoch 06 | train_loss=0.2427 val_loss=0.3260 val_f1=0.8864\n",
      "Epoch 07 | train_loss=0.2326 val_loss=0.3171 val_f1=0.8822\n",
      "Epoch 08 | train_loss=0.2221 val_loss=0.3192 val_f1=0.8874\n",
      "Epoch 09 | train_loss=0.2147 val_loss=0.3163 val_f1=0.8922\n",
      "Epoch 10 | train_loss=0.2081 val_loss=0.2982 val_f1=0.8923\n",
      "Epoch 11 | train_loss=0.2017 val_loss=0.3299 val_f1=0.8915\n",
      "Epoch 12 | train_loss=0.1955 val_loss=0.3021 val_f1=0.8957\n",
      "Epoch 13 | train_loss=0.1909 val_loss=0.3400 val_f1=0.8877\n"
     ]
    }
   ],
   "source": [
    "attn_gru_model = AttentiveGRUSiamese(len(vocab) + 2)\n",
    "start = time.time()\n",
    "attn_gru_model, attn_gru_history = train_model(\n",
    "    attn_gru_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epochs=EPOCHS,\n",
    "    lr=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    patience=PATIENCE\n",
    ")\n",
    "attn_gru_time = elapsed_since(start)\n",
    "attn_gru_test_loss, _ = run_epoch(attn_gru_model, test_loader)\n",
    "attn_val_probs, attn_val_targets = predict_probs(attn_gru_model, val_loader)\n",
    "attn_threshold = find_best_threshold(attn_val_probs, attn_val_targets)\n",
    "attn_test_probs, _ = predict_probs(attn_gru_model, test_loader)\n",
    "attn_gru_test_metrics = compute_metrics(attn_test_probs, y_test, threshold=attn_threshold)\n",
    "attn_gru_test_metrics['threshold'] = attn_threshold\n",
    "print(\"\\nAttentive GRU Test Loss:\", attn_gru_test_loss)\n",
    "print('Attentive GRU Test Metrics:', attn_gru_test_metrics)\n",
    "print(f\"Attentive GRU best threshold (val F1): {attn_threshold:.3f}\")\n",
    "\n",
    "histories['AttentiveGRU'] = attn_gru_history\n",
    "best_thresholds['AttentiveGRU'] = attn_threshold\n",
    "results['AttentiveGRU'] = {**attn_gru_test_metrics, 'loss': attn_gru_test_loss, 'train_time_sec': attn_gru_time}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94cd682",
   "metadata": {},
   "source": [
    "# 9. TF-IDF + Logistic Regression Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27ec120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Classical TF-IDF baseline ---\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "\n",
    "print(\"Training TF-IDF baseline (logistic regression)...\")\n",
    "tfidf_train_X = (train_df['text_a'] + \" [SEP] \" + train_df['text_b']).values\n",
    "tfidf_val_X = (val_df['text_a'] + \" [SEP] \" + val_df['text_b']).values\n",
    "tfidf_test_X = (test_df['text_a'] + \" [SEP] \" + test_df['text_b']).values\n",
    "\n",
    "best_c, best_f1 = 1.0, -1.0\n",
    "tfidf_model = None\n",
    "val_labels = val_df['label'].values\n",
    "for C in [0.1, 0.3, 1.0, 3.0, 10.0]:\n",
    "    pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(max_features=50000, ngram_range=(1, 2), lowercase=True, token_pattern=r\"[A-Za-z']+\")),\n",
    "        (\"clf\", LogisticRegression(max_iter=1000, C=C, solver=\"liblinear\"))\n",
    "    ])\n",
    "    pipe.fit(tfidf_train_X, train_df['label'].values)\n",
    "    val_probs = pipe.predict_proba(tfidf_val_X)[:, 1]\n",
    "    val_preds = (val_probs >= 0.5).astype(int)\n",
    "    _, _, f1, _ = precision_recall_fscore_support(val_labels, val_preds, average='binary', zero_division=0)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_c = C\n",
    "        tfidf_model = pipe\n",
    "\n",
    "print(f\"Best C on val: {best_c} (F1@0.5={best_f1:.4f})\")\n",
    "start = time.time()\n",
    "tfidf_model.fit(tfidf_train_X, train_df['label'].values)\n",
    "tfidf_time = elapsed_since(start)\n",
    "\n",
    "tfidf_val_probs = tfidf_model.predict_proba(tfidf_val_X)[:, 1]\n",
    "tfidf_threshold = find_best_threshold(tfidf_val_probs, val_labels)\n",
    "tfidf_test_probs = tfidf_model.predict_proba(tfidf_test_X)[:, 1]\n",
    "tfidf_metrics = compute_metrics(tfidf_test_probs, test_df['label'].values, threshold=tfidf_threshold)\n",
    "tfidf_metrics['threshold'] = tfidf_threshold\n",
    "print(f\"TF-IDF best threshold (val F1): {tfidf_threshold:.3f}\")\n",
    "print(\"TF-IDF baseline:\", tfidf_metrics)\n",
    "best_thresholds['TFIDF'] = tfidf_threshold\n",
    "results['TFIDF'] = {**tfidf_metrics, 'loss': float('nan'), 'train_time_sec': tfidf_time}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31b3c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cm_and_report(name, probs, labels, threshold):\n",
    "    preds = (probs >= threshold).astype(int)\n",
    "    print(f\"=== {name} (threshold={threshold:.3f}) Confusion Matrix ===\")\n",
    "    print(confusion_matrix(labels, preds))\n",
    "    print(f\"=== {name} Classification Report ===\")\n",
    "    print(classification_report(labels, preds, digits=4, zero_division=0))\n",
    "\n",
    "\n",
    "cm_and_report('BiLSTM', bilstm_test_probs, y_test, best_thresholds['BiLSTM'])\n",
    "cm_and_report('AttentiveGRU', attn_test_probs, y_test, best_thresholds['AttentiveGRU'])\n",
    "cm_and_report('TFIDF', tfidf_test_probs, y_test, best_thresholds['TFIDF'])\n",
    "\n",
    "res_df = pd.DataFrame(results).T[['accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'pr_auc', 'threshold', 'train_time_sec', 'loss']]\n",
    "display(res_df)\n",
    "\n",
    "roc_fig = plt.figure()\n",
    "for name, probs in [('TF-IDF', tfidf_test_probs), ('BiLSTM', bilstm_test_probs), ('AttentiveGRU', attn_test_probs)]:\n",
    "    fpr, tpr, _ = roc_curve(y_test, probs)\n",
    "    plt.plot(fpr, tpr, label=name)\n",
    "plt.plot([0, 1], [0, 1], '--')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC Curves')\n",
    "plt.legend()\n",
    "roc_fig.tight_layout()\n",
    "roc_fig.savefig('roc_curves.png', dpi=200, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "pr_fig = plt.figure()\n",
    "for name, probs in [('TF-IDF', tfidf_test_probs), ('BiLSTM', bilstm_test_probs), ('AttentiveGRU', attn_test_probs)]:\n",
    "    prec, rec, _ = precision_recall_curve(y_test, probs)\n",
    "    plt.plot(rec, prec, label=name)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('PR Curves')\n",
    "plt.legend()\n",
    "pr_fig.tight_layout()\n",
    "pr_fig.savefig('pr_curves.png', dpi=200, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60145fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Export data for report ---\n",
    "dataset_lines = [\n",
    "    f\"- Train: {split_stats['train']['clauses']:,} clauses across {split_stats['train']['labels']:,} labels ({pair_counts['train']:,} pairs)\",\n",
    "    f\"- Validation: {split_stats['val']['clauses']:,} clauses across {split_stats['val']['labels']:,} labels ({pair_counts['val']:,} pairs)\",\n",
    "    f\"- Test: {split_stats['test']['clauses']:,} clauses across {split_stats['test']['labels']:,} labels ({pair_counts['test']:,} pairs)\",\n",
    "    f\"- Negative/positive ratio per split: {NEG_POS_RATIO}:1\"\n",
    "]\n",
    "\n",
    "results_table = pd.DataFrame(results).T[\n",
    "    ['accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'pr_auc', 'threshold', 'train_time_sec']\n",
    "]\n",
    "\n",
    "try:\n",
    "    table_md = results_table.to_markdown()\n",
    "except Exception:\n",
    "    table_md = results_table.to_string()\n",
    "\n",
    "report = \"# Report: Legal Clause Similarity\\n\\n\"\n",
    "\n",
    "report += \"## Dataset & Splits\\n\"\n",
    "report += \"\\n\".join(dataset_lines) + \"\\n\\n\"\n",
    "\n",
    "report += \"## Architectures & Training\\n\"\n",
    "report += (\n",
    "    \"1. BiLSTM Siamese: Embedding(30k,128) -> BiLSTM(+/-128) -> mean pool -> LayerNorm+MLP.\\n\"\n",
    "    \"2. Attentive BiGRU Siamese: same embedding, BiGRU(+/-128) + additive attention -> MLP head.\\n\"\n",
    "    \"3. TF-IDF + Logistic Regression: 1-2 gram TF-IDF (50k max feats) + liblinear logistic (C tuned).\\n\\n\"\n",
    "    f\"Shared hyperparams: max_len={MAX_SEQ_LEN}, train_batch={TRAIN_BATCH_SIZE}, eval_batch={EVAL_BATCH_SIZE}, Adam lr={LR}, \"\n",
    "    f\"weight_decay={WEIGHT_DECAY}, epochs={EPOCHS}, patience={PATIENCE}, grad clip=2.0.\\n\"\n",
    ")\n",
    "\n",
    "report += \"## Test Metrics\\n\"\n",
    "report += table_md + \"\\n\\n\"\n",
    "\n",
    "report += \"ROC curve image: roc_curves.png | PR curve image: pr_curves.png\\n\\n\"\n",
    "\n",
    "report += \"## Error Patterns\\n\"\n",
    "report += (\n",
    "    \"- False positives: clauses sharing boilerplate but diverging in scope or carve-outs.\\n\"\n",
    "    \"- False negatives: paraphrases with modality shifts (\\\"may\\\" vs \\\"shall\\\") or rearranged conditions.\\n\\n\"\n",
    ")\n",
    "\n",
    "report += \"## Notes\\n\"\n",
    "report += \"Models, saved plots, and this report were generated without transformer backbones as required.\\n\"\n",
    "\n",
    "with open('exported.md', 'w', encoding='utf-8') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print('Wrote exported.md')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fd04e3",
   "metadata": {},
   "source": [
    "## 10. Training Dynamics & Result Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68eb863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_similarity(text_a: str, text_b: str, model, threshold=0.5):\n",
    "    model.eval()\n",
    "    a = torch.tensor([encode(text_a)], dtype=torch.long, device=device)\n",
    "    b = torch.tensor([encode(text_b)], dtype=torch.long, device=device)\n",
    "    with torch.no_grad():\n",
    "        prob = torch.sigmoid(model(a, b).squeeze(1)).item()\n",
    "    return {'prob_similar': prob, 'label_pred': int(prob >= threshold)}\n",
    "\n",
    "\n",
    "# demo\n",
    "print(infer_similarity(\n",
    "    \"This Agreement terminates if either party defaults.\",\n",
    "    \"If a party breaches, this Agreement may be terminated.\",\n",
    "    attn_gru_model\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a0f3dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T19:00:57.841582Z",
     "iopub.status.busy": "2025-11-08T19:00:57.841481Z",
     "iopub.status.idle": "2025-11-08T19:00:58.066544Z",
     "shell.execute_reply": "2025-11-08T19:00:58.066180Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "for name, history in histories.items():\n",
    "    epochs = [entry['epoch'] for entry in history]\n",
    "    axes[0].plot(epochs, [entry['train_loss'] for entry in history], label=f'{name} train')\n",
    "    axes[0].plot(epochs, [entry['val_loss'] for entry in history], linestyle='--', label=f'{name} val')\n",
    "    axes[1].plot(epochs, [entry['val_metrics']['f1'] for entry in history], label=name)\n",
    "axes[0].set_title('Loss per Epoch')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('BCE Loss')\n",
    "axes[1].set_title('Validation F1 per Epoch')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('F1 Score')\n",
    "axes[0].legend()\n",
    "axes[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df = results_df[['accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'pr_auc', 'threshold', 'loss', 'train_time_sec']]\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759752a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_qualitative_examples(model, df, left_arr, right_arr, labels_arr, model_name, threshold=0.5, max_examples=3):\n",
    "    model.eval()\n",
    "    samples = {\"correct\": [], \"incorrect\": []}\n",
    "    with torch.no_grad():\n",
    "        for idx in range(len(df)):\n",
    "            left = torch.tensor(left_arr[idx], dtype=torch.long, device=device).unsqueeze(0)\n",
    "            right = torch.tensor(right_arr[idx], dtype=torch.long, device=device).unsqueeze(0)\n",
    "            label = int(labels_arr[idx])\n",
    "            logit = model(left, right).squeeze(1)\n",
    "            prob = torch.sigmoid(logit).item()\n",
    "            pred = int(prob >= threshold)\n",
    "            bucket = \"correct\" if pred == label else \"incorrect\"\n",
    "            if len(samples[bucket]) < max_examples:\n",
    "                row = df.iloc[idx]\n",
    "                samples[bucket].append({\n",
    "                    \"text_a\": row['text_a'],\n",
    "                    \"text_b\": row['text_b'],\n",
    "                    \"true_label\": label,\n",
    "                    \"prediction\": pred,\n",
    "                    \"probability\": prob\n",
    "                })\n",
    "            if all(len(samples[key]) >= max_examples for key in samples):\n",
    "                break\n",
    "    print(f\"{model_name} qualitative samples (threshold={threshold:.2f})\")\n",
    "    for bucket in (\"correct\", \"incorrect\"):\n",
    "        print(f\"{bucket.title()} matches:\")\n",
    "        if not samples[bucket]:\n",
    "            print('  (no samples collected)')\n",
    "            continue\n",
    "        for item in samples[bucket]:\n",
    "            print(f\"  True label: {item['true_label']} | Pred: {item['prediction']} | Prob={item['probability']:.3f}\")\n",
    "            print('  Clause A:', item['text_a'][:400])\n",
    "            print('  Clause B:', item['text_b'][:400])\n",
    "            print('  ---')\n",
    "\n",
    "test_df_reset = test_df.reset_index(drop=True)\n",
    "collect_qualitative_examples(bilstm_model, test_df_reset, test_left, test_right, test_labels, 'BiLSTM Siamese', threshold=best_thresholds['BiLSTM'])\n",
    "collect_qualitative_examples(attn_gru_model, test_df_reset, test_left, test_right, test_labels, 'Attentive GRU Siamese', threshold=best_thresholds['AttentiveGRU'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1c1bf2",
   "metadata": {},
   "source": [
    "## 11. Observations\n",
    "\n",
    "- **Attentive BiGRU** consistently leads on F1/PR-AUC because attention sharpens focus on operative verbs (e.g., \"shall terminate\") and exception clauses, boosting recall on long paragraphs.\n",
    "- **BiLSTM** is ~3× faster to train and remains competitive on templated boilerplate but occasionally underweights decisive tokens buried deep in enumerations, hurting precision.\n",
    "- **TF-IDF** offers a cheap lexical baseline that excels on near-duplicates yet fails when semantically similar clauses use different vocabulary or when meaning hinges on cross-sentence structure.\n",
    "- Common false positives arise when both clauses share boilerplate but differ in scope (e.g., confidentiality carve-outs); false negatives typically involve modality changes (\"may\" vs \"shall\") or different ordering of conditions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
